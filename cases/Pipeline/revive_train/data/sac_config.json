{
    "base_config": [
        {
            "name": "global_seed",
            "abbreviation": "gs",
            "description": "Set the random number seed for the experiment.",
            "type": "int",
            "default": 42
        },
        {
            "name": "val_split_ratio",
            "abbreviation": "vsr",
            "description": "Ratio to split validate dataset if it is not explicitly given.",
            "type": "float",
            "default": 0.5
        },
        {
            "name": "val_split_mode",
            "abbreviation": "vsm",
            "description": "Mode of auto splitting training and validation dataset, choose from `outside_traj` and `inside_traj`. `outside_traj` means the split is happened outside the trajectories, one trajectory can only be in one dataset. `inside_traj` means the split is happened inside the trajectories, former part of one trajectory is in training set, later part is in validation set.",
            "type": "str",
            "default": "outside_traj"
        },
        {
            "name": "ignore_check",
            "abbreviation": "igc",
            "description": "Flag to ignore data related check, force training.",
            "type": "bool",
            "default": false
        },
        {
            "name": "use_time_step_embed",
            "abbreviation": "utse",
            "description": "Flag to use positional embedding for time step",
            "type": "bool",
            "default": true
        },
        {
            "name": "time_step_embed_size",
            "abbreviation": "tses",
            "description": "embedding size of positional embedding for time step",
            "type": "int",
            "default": 64
        },
        {
            "name": "use_traj_id_embed",
            "abbreviation": "utie",
            "description": "Flag to use binary embedding for trajetory id",
            "type": "bool",
            "default": true
        },
        {
            "name": "venv_rollout_horizon",
            "abbreviation": "vrh",
            "description": "Length of sampled trajectory, validate only if the algorithm works on sequential data.",
            "type": "int",
            "default": 20
        },
        {
            "name": "venv_gpus_per_worker",
            "abbreviation": "vgpw",
            "description": "Number of gpus per worker in venv training, small than 1 means launch multiple workers on the same gpu.",
            "type": "float",
            "default": 1.0
        },
        {
            "name": "venv_metric",
            "description": "Metric used to evaluate the trained venv, choose from `nll`, `mae`, `mse`, `wdist`.",
            "type": "str",
            "default": "mae"
        },
        {
            "name": "venv_algo",
            "description": "Algorithm used in venv training. There are currently three algorithms to choose from, `bc` and `revive_p`.",
            "type": "str",
            "default": "bc"
        },
        {
            "name": "rollout_plt_frequency",
            "abbreviation": "rpf",
            "description": "How many steps between two plot rollout data. 0 means disable.",
            "type": "int",
            "default": 50
        },
        {
            "name": "venv_save_frequency",
            "abbreviation": "vsp",
            "description": "How many epochs to save a model periodically. 0 means disable.",
            "type": "int",
            "default": 0
        },
        {
            "name": "rollout_dataset_mode",
            "description": "Select the rollout dataset. support `train` and `validate`",
            "type": "str",
            "default": "validate"
        },
        {
            "name": "policy_gpus_per_worker",
            "abbreviation": "pgpw",
            "description": "Number of gpus per worker in venv training, small than 1 means launch multiple workers on the same gpu.",
            "type": "float",
            "default": 1.0
        },
        {
            "name": "behavioral_policy_init",
            "abbreviation": "bpi",
            "description": "Whether to use the learned behavioral policy to as the initialization policy training.",
            "type": "bool",
            "default": true
        },
        {
            "name": "policy_algo",
            "description": "Algorithm used in policy training. There are currently two algorithms to choose from, `ppo` and `sac`.",
            "type": "str",
            "default": "sac"
        },
        {
            "name": "test_horizon",
            "abbreviation": "th",
            "description": "Rollout length of the venv test.",
            "type": "int",
            "default": 100
        },
        {
            "name": "train_venv_trials",
            "abbreviation": "tvt",
            "description": "Number of total trails searched by the search algorithm in venv training.",
            "type": "int",
            "default": 25
        },
        {
            "name": "train_policy_trials",
            "abbreviation": "tpt",
            "description": "Number of total trails searched by the search algorithm in policy training.",
            "type": "int",
            "default": 10
        }
    ],
    "venv_algo_config": {
        "revive_p": [
            {
                "name": "revive_batch_size",
                "description": "Batch size of training process.",
                "abbreviation": "mbs",
                "type": "int",
                "default": 1024
            },
            {
                "name": "revive_epoch",
                "description": "Number of epcoh for the training process",
                "abbreviation": "mep",
                "type": "int",
                "default": 5000
            },
            {
                "name": "fintune",
                "abbreviation": "bet",
                "type": "int",
                "default": 1
            },
            {
                "name": "finetune_fre",
                "abbreviation": "betfre",
                "type": "int",
                "default": 1
            },
            {
                "name": "policy_hidden_features",
                "description": "Number of neurons per layer of the policy network.",
                "abbreviation": "phf",
                "type": "int",
                "default": 256
            },
            {
                "name": "policy_hidden_layers",
                "description": "Depth of policy network.",
                "abbreviation": "phl",
                "type": "int",
                "default": 4
            },
            {
                "name": "policy_backbone",
                "description": "Backbone of policy network.",
                "abbreviation": "pb",
                "type": "str",
                "default": "res"
            },
            {
                "name": "transition_hidden_features",
                "description": "Number of neurons per layer of the transition network.",
                "abbreviation": "thf",
                "type": "int",
                "default": 256
            },
            {
                "name": "transition_hidden_layers",
                "abbreviation": "thl",
                "type": "int",
                "default": 4
            },
            {
                "name": "transition_backbone",
                "description": "Backbone of Transition network.",
                "abbreviation": "tb",
                "type": "str",
                "default": "res"
            },
            {
                "name": "matcher_hidden_features",
                "description": "Number of neurons per layer of the matcher network.",
                "abbreviation": "dhf",
                "type": "int",
                "default": 256
            },
            {
                "name": "matcher_hidden_layers",
                "description": "Depth of the matcher network.",
                "abbreviation": "dhl",
                "type": "int",
                "default": 4
            },
            {
                "name": "g_steps",
                "description": "The number of update rounds of the generator in each epoch.",
                "type": "int",
                "default": 1,
                "search_mode": "grid",
                "search_values": "[1,3,5]"
            },
            {
                "name": "d_steps",
                "description": "Number of update rounds of matcher in each epoch.",
                "type": "int",
                "default": 1,
                "search_mode": "grid",
                "search_values": "[1,3,5]"
            },
            {
                "name": "g_lr",
                "description": "Initial learning rate of the generator nodes nets.",
                "type": "float",
                "default": 4e-05,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.0001]"
            },
            {
                "name": "d_lr",
                "description": "Initial learning rate of the matcher.",
                "type": "float",
                "default": 0.0006,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.001]"
            },
            {
                "name": "bc_weight_decay",
                "description": "weight_decay in bc finetune",
                "type": "float",
                "default": 0.0001,
                "search_mode": "continuous",
                "search_values": "[1e-05,0.001]"
            }
        ],
        "bc": [
            {
                "name": "bc_batch_size",
                "description": "Batch size of training process.",
                "abbreviation": "bbs",
                "type": "int",
                "default": 256
            },
            {
                "name": "bc_epoch",
                "description": "Number of epcoh for the training process",
                "abbreviation": "bep",
                "type": "int",
                "default": 500
            },
            {
                "name": "policy_hidden_features",
                "description": "Number of neurons per layer of the policy network.",
                "abbreviation": "phf",
                "type": "int",
                "default": 256
            },
            {
                "name": "policy_hidden_layers",
                "description": "Depth of policy network.",
                "abbreviation": "phl",
                "type": "int",
                "default": 4,
                "search_mode": "grid",
                "search_values": "[3,4,5]"
            },
            {
                "name": "policy_backbone",
                "description": "Backbone of policy network.",
                "abbreviation": "pb",
                "type": "str",
                "default": "res",
                "search_mode": "grid",
                "search_values": "['mlp','res']"
            },
            {
                "name": "g_lr",
                "description": "Initial learning rate of the training process.",
                "type": "float",
                "default": 0.0001,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.001]"
            },
            {
                "name": "loss_type",
                "description": "Bc support different loss function(\"log_prob\", \"mae\", \"mse\").",
                "type": "str",
                "default": "nll"
            }
        ]
    },
    "policy_algo_config": {
        "ppo": [
            {
                "name": "ppo_batch_size",
                "description": "Batch size of training process.",
                "abbreviation": "pbs",
                "type": "int",
                "default": 256
            },
            {
                "name": "ppo_epoch",
                "description": "Number of epcoh for the training process",
                "abbreviation": "bep",
                "type": "int",
                "default": 200
            },
            {
                "name": "ppo_rollout_horizon",
                "description": "Rollout length of the policy train.",
                "abbreviation": "prh",
                "type": "int",
                "default": 100
            },
            {
                "name": "policy_hidden_features",
                "description": "Number of neurons per layer of the policy network.",
                "abbreviation": "phf",
                "type": "int",
                "default": 256
            },
            {
                "name": "policy_hidden_layers",
                "description": "Depth of policy network.",
                "abbreviation": "phl",
                "type": "int",
                "default": 4
            },
            {
                "name": "policy_backbone",
                "description": "Backbone of policy network.",
                "abbreviation": "pb",
                "type": "str",
                "default": "mlp"
            },
            {
                "name": "g_lr",
                "description": "Initial learning rate of the training process.",
                "type": "float",
                "default": 4e-05,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.001]"
            }
        ],
        "sac": [
            {
                "name": "sac_batch_size",
                "description": "Batch size of training process.",
                "abbreviation": "pbs",
                "type": "int",
                "default": 1024
            },
            {
                "name": "sac_epoch",
                "description": "Number of epcoh for the training process.",
                "abbreviation": "bep",
                "type": "int",
                "default": 500
            },
            {
                "name": "sac_steps_per_epoch",
                "description": "The number of update rounds of sac in each epoch.",
                "abbreviation": "sspe",
                "type": "int",
                "default": 200
            },
            {
                "name": "sac_rollout_horizon",
                "abbreviation": "srh",
                "type": "int",
                "default": 50
            },
            {
                "name": "policy_hidden_features",
                "description": "Number of neurons per layer of the policy network.",
                "abbreviation": "phf",
                "type": "int",
                "default": 256
            },
            {
                "name": "policy_hidden_layers",
                "description": "Depth of policy network.",
                "abbreviation": "phl",
                "type": "int",
                "default": 4
            },
            {
                "name": "policy_backbone",
                "description": "Backbone of policy network.",
                "abbreviation": "pb",
                "type": "str",
                "default": "mlp"
            },
            {
                "name": "policy_hidden_activation",
                "description": "hidden_activation of policy network.",
                "abbreviation": "pha",
                "type": "str",
                "default": "leakyrelu"
            },
            {
                "name": "buffer_size",
                "description": "Size of the buffer to store data.",
                "abbreviation": "bfs",
                "type": "int",
                "default": 1000000.0
            },
            {
                "name": "g_lr",
                "description": "Initial learning rate of the training process.",
                "type": "float",
                "default": 4e-05,
                "search_mode": "continuous",
                "search_values": "[1e-06,0.001]"
            }
        ]
    }
}